---
title: Appendix. Probability Theory: An advanced approach.
author: Phuoc-Truong Huynh
date: 2021-07-15 12:00:00 +0200
categories: [Seminar]
tags: [inverse problems, bayesian, optimization]
toc: true
math: true
---

## **Basic concepts**

A **probability space** is a triplet $$(\Omega, \mathcal{F}, \mathbb{P})$$, where $$\Omega$$ is the sample space, $$\mathcal{F}$$ is a $$\sigma-$$ algebra of events and $$\mathbb{P}$$ is a probability measure on $$\Omega$$.

Let $$(E,\mathbb{B}(E))$$ be a measurable space, where $$\mathbb{B}(E)$$ denotes the Borel $$\sigma-$$ algebra generated by the open sets of $$E$$.

A measurable mapping $$u: \Omega \to E$$ is a random variable. This random variable induces a probability measure $$\mu$$ on $$E$$ given by

$$\mu(A):= \mathbb{P}(u^{-1}(A)) = \mathbb{P}\left(\left\{\omega \in \Omega: u(\omega) \in A\right\}\right),\quad A \in \mathbb{B}(E).$$

In this case, the measure $$\mu$$ is called the **probability distribution** of $$u$$ and we write $$u \sim \mu$$.

In this note, we use the notations $$X,Y, u, y, \ldots$$ to denote random variables.



(We note that $$\mu$$ is also called the push-forward of $$\mathbb{P}$$ via $$u$$ and is denoted by $$u_*\mathbb{P}$$.)

We write
$$\int X(\omega) d \mathbb{P}(\omega), \text{ or } \int X(\omega) \mathbb{P}(d\omega).$$

**Theorem.**

Let $$(\Omega_1, \mathcal{F}_1)$$ and $$(\Omega_2,\mathcal{F}_2)$$ be measurable spaces and let $$\mu$$ be a measure on $$(\Omega_1, \mathcal{F}_1)$$. Assume that $$X: \Omega_1 \to \Omega_2$$ is a measurable map and $$X_*\mu$$ the push-forward measure. If $$f \in L^1(\Omega_2,d X_* \mu)$$, then $$f \circ X \in L^1(\Omega_1,d\mu)$$ and

$$\int_{\Omega_1}(f \circ X) d\mu = \int_{\Omega_2} f d(X_* \mu).$$

In particular, if $$X$$ is a random variable on $$(\Omega, \mathcal{F}, \mathbb{P})$$, then

$$\int f(x) \mathbb{P}(X \in dx):= \int f(x) dX_*\mathbb{P}(x) = \int f\circ X(\omega) d P(\omega).$$

## **Conditional probability and conditional distribution**

Let $$u,y: \Omega \to X$$ be two random variables. We first consider a simple case where $$X = \mathbb{R}^n$$. The conditional probability distribution of $$Y$$ given $$X$$ is a Markov kernel
