I"±<h2 id="basic-concepts"><strong>Basic concepts</strong></h2>

<p>A <strong>probability space</strong> is a triplet \((\Omega, \mathcal{F}, \mathbb{P})\), where \(\Omega\) is the sample space, \(\mathcal{F}\) is a \(\sigma-\) algebra of events and \(\mathbb{P}\) is a probability measure on \(\Omega\).</p>

<p>Let \((E,\mathbb{B}(E))\) be a measurable space, where \(\mathbb{B}(E)\) denotes the Borel \(\sigma-\) algebra generated by the open sets of \(E\).</p>

<p>A measurable mapping \(X: \Omega \to E\) is a random variable. This random variable induces a probability measure \(\mu\) on \(E\) given by</p>

\[\mu_X(A):= \mathbb{P}(X^{-1}(A)) = \mathbb{P}\left(\left\{\omega \in \Omega: X(\omega) \in A\right\}\right),\quad A \in \mathbb{B}(E).\]

<p>The measure \(\mu_X\) is called the <strong>probability distribution</strong> of \(u\) and we write \(X \sim \mu_X\). We note that \(\mu_X\) is also called the push-forward of \(\mathbb{P}\) via \(X\) and is denoted by \(X_*\mathbb{P}\).</p>

<p>In this note, we use the notations \(X,Y, u, y, \ldots\) to denote random variables.</p>

<p>We write</p>

\[\int X(\omega) d \mathbb{P}(\omega), \text{ or } \int X(\omega) \mathbb{P}(d\omega).\]

<p><strong>Theorem.</strong></p>

<p>Let \((\Omega_1, \mathcal{F}_1)\) and \((\Omega_2,\mathcal{F}_2)\) be measurable spaces and let \(\mu\) be a measure on \((\Omega_1, \mathcal{F}_1)\). Assume that \(X: \Omega_1 \to \Omega_2\) is a measurable map and \(X_*\mu\) the push-forward measure. If \(f \in L^1(\Omega_2,d X_* \mu)\), then \(f \circ X \in L^1(\Omega_1,d\mu)\) and</p>

\[\int_{\Omega_1}(f \circ X) d\mu = \int_{\Omega_2} f d(X_* \mu).\]

<p>In particular, if \(X\) is a random variable on \((\Omega, \mathcal{F}, \mathbb{P})\), then</p>

\[\int f(x) \mathbb{P}(X \in dx):= \int f(x) dX_*\mathbb{P}(x) = \int f\circ X(\omega) d P(\omega).\]

<p><strong>Proposition.</strong>
Let \(\mu_1\) and \(\mu_2\) be two \(\sigma-\) finite measures on \(\mathbb{R}^n\) defined over the Borel \(\sigma-\) algebra of \(\mathbb{R}^n\) such that \(\mu_1 \ll \mu_2\). Then there exists a measurable function \(\pi: \mathbb{R}^n \to \mathbb{R}\) such that
\(\mu_1(B) = \int_{B} \pi(x)d\mu_2(x),\quad \forall B \in \mathbb{B}(\mathbb{R^n}).\)</p>

<p>The function \(\pi\) is called the Radon-Nikodym derivative of \(\mu_1\) with respect to \(\mu_2\), denoted by</p>

\[\pi = \dfrac{d\mu_1}{d\mu_2}.\]

<p>It can be seen that</p>

<h2 id="joint-probability-distributions"><strong>Joint probability distributions.</strong></h2>

<p>Let \(X_1 : \Omega \to \mathbb{R}^m\) and \(X_2 : \Omega \to \mathbb{R}^n\) be two random variables. The joint probability distribution of \((X_1,X_2)\) is the probability distribution \(\mu_{X_1 X_2}\) of the product random variables
\(X_1 \times X_2: \Omega \to \mathbb{R}^m \times \mathbb{R}^n, \quad \omega \mapsto (X_1(\omega),X_2(\omega)),\)
i.e.
\(\mu_{X_1X_2}(B_1,B_2) = \mathbb{P}(X_1^{-1}(B_1) \cap X_2^{-1}(B_2))\)
for all \(B_1 \in \mathbb{B}(\mathbb{R^m}), B_2 \in \mathbb{B}(\mathbb{R^n}).\)</p>

<h2 id="conditional-probabilities-and-conditional-distributions"><strong>Conditional probabilities and conditional distributions.</strong></h2>

<p>Let \(X_1 : \Omega \to \mathbb{R}^m\) and \(X_2 : \Omega \to \mathbb{R}^n\) be two random variables. Denote by \(\mu_{X_1X_2}\) the joint probability distribution of \((X_1,X_2)\). The aim of this section is to find the probability distribution of \(X_2\) when \(X_1\) is given.</p>

<p>Assume that \(\mu_{X_1X_2}\) admits a density function \(\pi_{X_1 X_2}: \mathbb{R}^m \times \mathbb{R}^n \to \mathbb{R}\). We have</p>

\[\mu_{X_1 X_2}(B_1,B_2) = \int_{B_1 \times B_2} \pi_{X_1 X_2}(x_1,x_2)d x_1 dx_2, \quad \forall B_1 \times B_2 \in \mathbb{B}(\mathbb{R}^m)\times \mathbb{B}(\mathbb{R}^n).\]

<p><strong>Definition.</strong> The marginal density of \(X_1\) is the the probability distribution of \(X_1\) when \(X_2\) may take on any value, i.e.,</p>

\[\mu_{X_1}(B_1):= \mu_{X_1 X_2}(B_1,\mathbb{R}^n),\quad \forall B_1 \in \mathbb{B}(\mathbb{R^m}).\]

<p>In this case, one has</p>

\[\mu_{X_1}(B_1):= \int_{B_1} \int_{\mathbb{R}^n} \pi_{X_1X_2}(x_1,x_2)dx_2 dx_1. \label{eq:eq1}\tag{1}\]

<p>If we denoted</p>

\[\pi_{X_1}(x_1):= \int_{\mathbb{R}^n} \pi_{X_1 X_2} (x_1,x_2) dx_2,\]

<p>then by \eqref{eq:eq1} can write
\(\mu_{X_1}(B_1) = \int_{B_1} \pi_{X_1}(x_1)d x_1, \quad \forall B_1 \in \mathbb{B}(\mathbb{R}^m).\)</p>

<p>In other words, \(\pi_{X_1}\) is the probability density function of \(\mu_{X_1}\).</p>

<p>We define the conditional measure of \(B_1\) conditioned on \(B_2\), where \(\mu_{X_2}(B_2) = \mathbb{P}(X_2^{-1}(B_2))&gt;0\), by</p>

\[\begin{aligned}
\mu_{X_1|X_2}: \mathbb{B}(\mathbb{R}^m) \times \mathbb{B}(\mathbb{R}^n) &amp;\to [0,1] \\
(B_1,B_2) &amp;\mapsto \mu_{X_1|X_2}(B_1|B_2) := \dfrac{\mu_{X_1X_2}(B_1,B_2)}{\mu_{X_2}(B_2)}.
\end{aligned}\]

<blockquote>
  <p><strong>Overthinking</strong>.
We note that with a fixed \(B_2\) with \(\mu_{X_2}(B_2)&gt; 0\), the map \(B_1 \mapsto \mu_{X_1|X_2} (B_1|B_2)\) is a probability measure on \(\mathbb{R}^m\) since</p>

\[0\le \mu_{X_1X_2}(B_1,B_2) \le \mu_{X_1X_2}(\mathbb{R}^m,B_2) =  \mu_{X_2}(B_2),\]

  <p>by the definition of \(\mu_{X_2}(B_2)\).</p>
</blockquote>

<p>Assume that \(X_1,X_2\) are absolutely continuous. We find a function \(f(\cdot,B_2)\) such that
\(\mu_{X_1|X_2} (B_1|B_2) = \int_{B_1} f(x_1,B_2)dx_1.\)</p>

<p>In this case, the function \(f(\cdot,B_2)\) is called the density function of \(X_1\) conditioned on \(B_2\).</p>

<table>
  <tbody>
    <tr>
      <td>Indeed, by the definition of $$\mu_{X_1</td>
      <td>X_2}$$, one has</td>
      <td>Â </td>
    </tr>
    <tr>
      <td>$$\mu_{X_1</td>
      <td>X_2}(B_1</td>
      <td>B_2) = \dfrac{1}{\mu_{X_2}(B_2)} \int_{B_1} \left(\int_{B_2} \pi_{X_1X_2}(x_1,x_2)dx_2\right)dx_1.$$</td>
    </tr>
  </tbody>
</table>

<p>If we denote</p>

\[\pi_{X_1|B_2}(x_1):= \dfrac{1}{\mu_{X_2}(B_2)} \int_{B_2} \pi_{X_1X_2}(x_1,x_2)dx_2,\]

<p>then</p>

\[\mu_{X_1|X_2}(B_1|B_2) = \int_{B_1} \pi_{X_1|B_2}(x_1)dx_1.\]

<p>Hence, $\pi</p>
:ET